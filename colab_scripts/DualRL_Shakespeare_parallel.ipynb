{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "DualRL_Shakespeare_parallel.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1KbhS5enaDYTFn8Fo_bmSn9ADJXqXwM73",
      "authorship_tag": "ABX9TyPaHfpJl7bo++ZKe8n7G2CL",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/magdalena-b/Bairon/blob/master/DualRL_Shakespeare_parallel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UevsVzCPs12d",
        "outputId": "447c3f01-5b5a-44d6-ce49-e0333668bc41"
      },
      "source": [
        "!pip install OpenNMT-tf==1.15.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: OpenNMT-tf==1.15.0 in /usr/local/lib/python3.7/dist-packages (1.15.0)\n",
            "Requirement already satisfied: pyonmttok<2,>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from OpenNMT-tf==1.15.0) (1.26.4)\n",
            "Requirement already satisfied: rouge==0.3.1 in /usr/local/lib/python3.7/dist-packages (from OpenNMT-tf==1.15.0) (0.3.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from OpenNMT-tf==1.15.0) (3.13)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScBRJd6ztCDG",
        "outputId": "a4ce9f19-0f37-41f3-febd-f0e7fd671d5b"
      },
      "source": [
        "!pip install tensorflow==1.13.1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==1.13.1 in /usr/local/lib/python3.7/dist-packages (1.13.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.39.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (3.17.3)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.4.0)\n",
            "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.13.1)\n",
            "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.13.0)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.12.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.0.8)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.37.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.1.2)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.19.5)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.8.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.1) (3.1.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.3.4)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (4.6.4)\n",
            "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow==1.13.1) (4.0.3)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.6->tensorflow==1.13.1) (1.5.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.7.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMVvSZlee0sZ",
        "outputId": "75a96c57-0afd-46bd-c89c-a8df0375738b"
      },
      "source": [
        "%env DATASET=yelp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "env: DATASET=yelp\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLPea3SLmxlX",
        "outputId": "a05582b4-c510-4ea6-ea0c-05262b1fef02"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7kcDRwEuzjB",
        "outputId": "4f006085-06de-4437-e1bb-6b4c84ab0938"
      },
      "source": [
        "%cd drive/MyDrive/DualRL"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/DualRL\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGYqp_0jpSA0"
      },
      "source": [
        "import sys\n",
        "sys.path.insert(0,'/content/drive/MyDrive/DualRL')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6u1YI7WpaUr"
      },
      "source": [
        "import utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idAfgQveJldF",
        "outputId": "41585e24-439f-48bb-9b6c-db3ae378e56a"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "classifier\t   dual_options.py   LICENSE  __pycache__  tmp\n",
            "common_options.py  dual_training.py  nmt      README.md    utils\n",
            "data\t\t   fig\t\t     outputs  references\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVLWr_rLsIjZ"
      },
      "source": [
        "import classifier.options"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUa64HIEpvOT",
        "outputId": "4895f901-8815-44cb-8e15-f1b5401559a5"
      },
      "source": [
        "from classifier.model import TextCNN"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qd2wvBsa4YQP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e82d58ca-de31-47ab-c896-2aab7bba49bc"
      },
      "source": [
        "!pip install easydict"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: easydict in /usr/local/lib/python3.7/dist-packages (1.9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTmmqMmMpoR7",
        "outputId": "6ea79588-a98c-43b3-ce90-90dea29900a9"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import re\n",
        "import time\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "from classifier.model import TextCNN\n",
        "from classifier.options import *\n",
        "from utils import constants\n",
        "from utils.vocab import build_vocab_from_file, load_vocab_dict\n",
        "import os\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2,1,0,3\"\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "\n",
        "def create_model(sess, args, vocab_size, mode=constants.TRAIN, load_pretrained_model=False, reuse=None):\n",
        "    with tf.variable_scope(constants.CLS_VAR_SCOPE, reuse=reuse):\n",
        "        model = TextCNN(mode, args.__dict__, vocab_size)\n",
        "\n",
        "    if load_pretrained_model:\n",
        "        try:\n",
        "            model.saver.restore(sess, args.cls_model_save_dir)\n",
        "            print(\"Loading model from\", args.cls_model_save_dir)\n",
        "        except Exception as e:\n",
        "            model.saver.restore(sess, tf.train.latest_checkpoint(args.cls_model_save_dir))\n",
        "            print(\"Loading model from\", tf.train.latest_checkpoint(args.cls_model_save_dir))\n",
        "    else:\n",
        "        if reuse is None:\n",
        "            print(\"Creating model with new parameters.\")\n",
        "            sess.run(tf.global_variables_initializer())\n",
        "        else:\n",
        "            print('Reuse parameters.')\n",
        "    return model\n",
        "\n",
        "\n",
        "def evaluate(sess, args, vocab, model, x, y, print_logs=True):\n",
        "    probs = []\n",
        "    batches = get_batches(x, y, word2id=vocab, batch_size=1)\n",
        "    for batch in batches:\n",
        "        p = sess.run(model.probs,\n",
        "                     feed_dict={model.x: batch[\"x\"],\n",
        "                                model.dropout: 1})\n",
        "        probs += p.tolist()\n",
        "    y_hat = [p > 0.5 for p in probs]\n",
        "    same = [p == q for p, q in zip(y, y_hat)]\n",
        "\n",
        "    if print_logs:\n",
        "        print(\"Saving classifier result at: %s\" % args.log_path)\n",
        "    with open(args.log_path, 'w') as f:\n",
        "        for i in range(len(y)):\n",
        "            f.write(\"%s\\t%.3f\\t%s\\n\" % (' '.join(x[i]), probs[i], same[i]))\n",
        "    return (100.0 * sum(same)) / len(y), probs\n",
        "\n",
        "\n",
        "def get_batches(x, y, word2id, batch_size, min_len=5):\n",
        "    pad = word2id[constants.PADDING_TOKEN]\n",
        "    unk = word2id[constants.UNKNOWN_TOKEN]\n",
        "\n",
        "    batches = []\n",
        "    s = 0\n",
        "    sen_len = []\n",
        "    while s < len(x):\n",
        "        t = min(s + batch_size, len(x))\n",
        "\n",
        "        _x = []\n",
        "        max_len = max([len(sent) for sent in x[s:t]])\n",
        "        max_len = max(max_len, min_len)  # sensitive to sentence-length\n",
        "        sen_len.append(max_len)\n",
        "        for sent in x[s:t]:\n",
        "            sent_id = [word2id[w] if w in word2id else unk for w in sent]\n",
        "            padding = [pad] * (max_len - len(sent))\n",
        "            _x.append(padding + sent_id)\n",
        "\n",
        "        batches.append({\"x\": _x,\n",
        "                        \"y\": y[s:t]})\n",
        "        s = t\n",
        "    return batches\n",
        "\n",
        "\n",
        "def prepare(paths, index_list=None, is_training=False):\n",
        "    def load_sent(path, max_size=-1):\n",
        "        data = []\n",
        "        with open(path) as f:\n",
        "            for line in f:\n",
        "                if len(data) == max_size:\n",
        "                    break\n",
        "                words = line.split()\n",
        "                if is_training:\n",
        "                    if len(words) > 1:  # filter sentence only have one words\n",
        "                        data.append(words)\n",
        "                else:\n",
        "                    data.append(words)\n",
        "        return data\n",
        "\n",
        "    if index_list is None:\n",
        "        index_list = []\n",
        "        for path in paths:\n",
        "            i = int(re.findall('\\d', path)[-1])\n",
        "            if '.tsf' in path or 'reference' in path:\n",
        "                i = 1-i\n",
        "            index_list.append(i)\n",
        "\n",
        "    data0 = load_sent(paths[0])\n",
        "    if len(paths) >= 2:\n",
        "        data1 = load_sent(paths[1])\n",
        "        if is_training:\n",
        "            min_c = min(len(data0), len(data1))\n",
        "            np.random.shuffle(data0)\n",
        "            np.random.shuffle(data1)\n",
        "            # balance the bias caused by training data\n",
        "            print(\"Dropped: %d, %s\" % (len(data0) - min_c, paths[0]))\n",
        "            data0 = data0[:min_c]\n",
        "            print(\"Dropped: %d, %s\" % (len(data1) - min_c, paths[1]))\n",
        "            data1 = data1[:min_c]\n",
        "\n",
        "        x = data0 + data1\n",
        "        y = [index_list[0]] * len(data0) + [index_list[1]] * len(data1)\n",
        "    else:\n",
        "        x = data0\n",
        "        y = [index_list[0]] * len(data0)\n",
        "\n",
        "    z = sorted(zip(x, y), key=lambda i: len(i[0]))  # ranked by the length of sentences\n",
        "    return zip(*z)\n",
        "\n",
        "\n",
        "def evaluate_file(sess, args, vocab, eval_model, files, index_list, print_logs=True):\n",
        "    x, y = prepare(files, index_list=index_list)\n",
        "    acc, _ = evaluate(sess, args, vocab, eval_model, x, y, print_logs)\n",
        "    return acc\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    args = load_cls_arguments()\n",
        "\n",
        "    if args.train_data and args.mode == constants.TRAIN:\n",
        "        train_x, train_y = prepare(args.train_data, index_list=[0, 1], is_training=True)\n",
        "\n",
        "        if not os.path.isfile(args.global_vocab_file):\n",
        "            build_vocab_from_file(args.train_data, args.global_vocab_file)\n",
        "\n",
        "    vocab, vocab_size = load_vocab_dict(args.global_vocab_file)\n",
        "    print(\"Vocabulary size\", vocab_size)\n",
        "\n",
        "    if args.dev_data:\n",
        "        dev_x, dev_y = prepare(args.dev_data)\n",
        "\n",
        "    # if args.test_data and args.mode == constants.EVAL:\n",
        "    if args.test_data:\n",
        "        test_x, test_y = prepare(args.test_data)\n",
        "\n",
        "    print(\"Prepare to Save model at: %s\" % args.cls_model_save_dir)\n",
        "    if not os.path.exists(args.cls_model_save_dir):\n",
        "        os.makedirs(args.cls_model_save_dir)\n",
        "\n",
        "    dump_args_to_yaml(args, args.cls_model_save_dir)\n",
        "\n",
        "    config = tf.ConfigProto()\n",
        "    # config.gpu_options.allow_growth = True\n",
        "    # config.gpu_options.per_process_gpu_memory_fraction = 0.2\n",
        "    with tf.Session(config=config) as sess:\n",
        "        with tf.device(\"/cpu:0\"):   # gpu will face error\n",
        "            if args.mode == constants.TRAIN:\n",
        "                model = create_model(sess, args, vocab_size)\n",
        "                if args.train_data:\n",
        "                    batches = get_batches(train_x, train_y,\n",
        "                                          vocab, args.batch_size)\n",
        "                    random.shuffle(batches)\n",
        "\n",
        "                    start_time = time.time()\n",
        "                    step = 0\n",
        "                    loss = 0.0\n",
        "                    best_dev = float(\"-inf\")\n",
        "                    learning_rate = args.learning_rate\n",
        "\n",
        "                    for epoch in range(1, 1 + args.n_epoch):\n",
        "                        print(\"--------------------Epoch %d--------------------\" % epoch)\n",
        "\n",
        "                        for batch in batches:\n",
        "                            step_loss, _ = sess.run([model.loss, model.optimizer],\n",
        "                                                    feed_dict={model.x: batch[\"x\"],\n",
        "                                                               model.y: batch[\"y\"],\n",
        "                                                               model.dropout: args.keep_prob})\n",
        "\n",
        "                            step += 1\n",
        "                            loss += step_loss / args.steps_per_checkpoint\n",
        "\n",
        "                            if step % args.steps_per_checkpoint == 0:\n",
        "                                print(\"step %d, time %.0fs, loss %.2f\" % (step, time.time() - start_time, loss))\n",
        "                                loss = 0.0\n",
        "\n",
        "                        if args.dev_data:\n",
        "                            acc, _ = evaluate(sess, args, vocab, model, dev_x, dev_y)\n",
        "                            print(\"Dev accuracy: %.2f\" % acc)\n",
        "                            if acc > best_dev:\n",
        "                                best_dev = acc\n",
        "                                print(\"Saving model to:  %s\" % args.cls_model_save_dir)\n",
        "                                model.saver.save(sess, args.cls_model_save_dir)\n",
        "                        if args.test_data:\n",
        "                            acc, _ = evaluate(sess, args, vocab, model, test_x, test_y)\n",
        "                            print(\"Test accuracy: %.2f\" % acc)\n",
        "            else:\n",
        "                if args.test_data:\n",
        "                    eval_model = create_model(sess, args, vocab_size, load_pretrained_model=True)\n",
        "                    acc, _ = evaluate(sess, args, vocab, eval_model, test_x, test_y)\n",
        "                    print(\"Test accuracy: %.2f\" % acc)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "Dropped: 0, /content/drive/MyDrive/DualRL/data/yelp/train.0\n",
            "Dropped: 88887, /content/drive/MyDrive/DualRL/data/yelp/train.1\n",
            "Vocabulary size 9353\n",
            "Prepare to Save model at: /content/drive/MyDrive/DualRL/tmp/model/yelp/cls/\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /content/drive/MyDrive/DualRL/classifier/model.py:56: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "Creating model with new parameters.\n",
            "--------------------Epoch 1--------------------\n",
            "step 100, time 5s, loss 0.65\n",
            "step 200, time 10s, loss 0.28\n",
            "step 300, time 15s, loss 0.22\n",
            "step 400, time 20s, loss 0.22\n",
            "step 500, time 25s, loss 0.19\n",
            "step 600, time 30s, loss 0.14\n",
            "step 700, time 34s, loss 0.14\n",
            "step 800, time 39s, loss 0.14\n",
            "step 900, time 44s, loss 0.15\n",
            "step 1000, time 49s, loss 0.18\n",
            "step 1100, time 54s, loss 0.17\n",
            "step 1200, time 59s, loss 0.14\n",
            "step 1300, time 64s, loss 0.14\n",
            "step 1400, time 68s, loss 0.14\n",
            "step 1500, time 73s, loss 0.12\n",
            "step 1600, time 78s, loss 0.13\n",
            "step 1700, time 83s, loss 0.13\n",
            "step 1800, time 87s, loss 0.09\n",
            "step 1900, time 92s, loss 0.12\n",
            "step 2000, time 97s, loss 0.12\n",
            "step 2100, time 102s, loss 0.14\n",
            "step 2200, time 107s, loss 0.10\n",
            "step 2300, time 112s, loss 0.11\n",
            "step 2400, time 116s, loss 0.11\n",
            "step 2500, time 121s, loss 0.11\n",
            "step 2600, time 126s, loss 0.11\n",
            "step 2700, time 131s, loss 0.10\n",
            "step 2800, time 136s, loss 0.11\n",
            "step 2900, time 140s, loss 0.10\n",
            "step 3000, time 145s, loss 0.12\n",
            "step 3100, time 150s, loss 0.11\n",
            "step 3200, time 155s, loss 0.10\n",
            "step 3300, time 160s, loss 0.11\n",
            "step 3400, time 165s, loss 0.11\n",
            "step 3500, time 170s, loss 0.13\n",
            "step 3600, time 175s, loss 0.10\n",
            "step 3700, time 180s, loss 0.13\n",
            "step 3800, time 185s, loss 0.11\n",
            "step 3900, time 190s, loss 0.11\n",
            "step 4000, time 194s, loss 0.13\n",
            "step 4100, time 199s, loss 0.12\n",
            "step 4200, time 204s, loss 0.11\n",
            "step 4300, time 209s, loss 0.11\n",
            "step 4400, time 214s, loss 0.09\n",
            "step 4500, time 219s, loss 0.10\n",
            "step 4600, time 223s, loss 0.12\n",
            "step 4700, time 228s, loss 0.09\n",
            "step 4800, time 233s, loss 0.11\n",
            "step 4900, time 238s, loss 0.09\n",
            "step 5000, time 243s, loss 0.10\n",
            "step 5100, time 247s, loss 0.08\n",
            "step 5200, time 252s, loss 0.10\n",
            "step 5300, time 257s, loss 0.11\n",
            "step 5400, time 262s, loss 0.13\n",
            "step 5500, time 267s, loss 0.10\n",
            "step 5600, time 272s, loss 0.10\n",
            "step 5700, time 276s, loss 0.12\n",
            "step 5800, time 281s, loss 0.08\n",
            "step 5900, time 286s, loss 0.11\n",
            "step 6000, time 291s, loss 0.10\n",
            "step 6100, time 296s, loss 0.09\n",
            "step 6200, time 300s, loss 0.09\n",
            "step 6300, time 305s, loss 0.09\n",
            "step 6400, time 310s, loss 0.10\n",
            "step 6500, time 315s, loss 0.11\n",
            "step 6600, time 319s, loss 0.08\n",
            "step 6700, time 324s, loss 0.09\n",
            "step 6800, time 329s, loss 0.09\n",
            "step 6900, time 334s, loss 0.13\n",
            "step 7000, time 339s, loss 0.08\n",
            "step 7100, time 344s, loss 0.11\n",
            "step 7200, time 348s, loss 0.08\n",
            "step 7300, time 353s, loss 0.10\n",
            "step 7400, time 358s, loss 0.09\n",
            "step 7500, time 363s, loss 0.11\n",
            "step 7600, time 368s, loss 0.09\n",
            "step 7700, time 373s, loss 0.10\n",
            "step 7800, time 377s, loss 0.08\n",
            "step 7900, time 382s, loss 0.09\n",
            "step 8000, time 387s, loss 0.09\n",
            "step 8100, time 392s, loss 0.07\n",
            "step 8200, time 397s, loss 0.09\n",
            "step 8300, time 402s, loss 0.08\n",
            "step 8400, time 406s, loss 0.09\n",
            "step 8500, time 411s, loss 0.07\n",
            "step 8600, time 416s, loss 0.10\n",
            "step 8700, time 421s, loss 0.07\n",
            "step 8800, time 426s, loss 0.05\n",
            "step 8900, time 430s, loss 0.10\n",
            "step 9000, time 435s, loss 0.08\n",
            "step 9100, time 440s, loss 0.08\n",
            "step 9200, time 445s, loss 0.08\n",
            "step 9300, time 450s, loss 0.08\n",
            "step 9400, time 454s, loss 0.07\n",
            "step 9500, time 459s, loss 0.09\n",
            "step 9600, time 464s, loss 0.09\n",
            "step 9700, time 468s, loss 0.10\n",
            "step 9800, time 473s, loss 0.08\n",
            "step 9900, time 478s, loss 0.09\n",
            "step 10000, time 483s, loss 0.10\n",
            "step 10100, time 488s, loss 0.09\n",
            "step 10200, time 493s, loss 0.11\n",
            "step 10300, time 498s, loss 0.09\n",
            "step 10400, time 503s, loss 0.07\n",
            "step 10500, time 508s, loss 0.09\n",
            "step 10600, time 513s, loss 0.10\n",
            "step 10700, time 517s, loss 0.09\n",
            "step 10800, time 522s, loss 0.10\n",
            "step 10900, time 527s, loss 0.07\n",
            "step 11000, time 532s, loss 0.09\n",
            "step 11100, time 537s, loss 0.10\n",
            "step 11200, time 542s, loss 0.07\n",
            "step 11300, time 547s, loss 0.09\n",
            "step 11400, time 552s, loss 0.08\n",
            "step 11500, time 557s, loss 0.08\n",
            "step 11600, time 562s, loss 0.07\n",
            "step 11700, time 566s, loss 0.09\n",
            "step 11800, time 571s, loss 0.08\n",
            "step 11900, time 576s, loss 0.08\n",
            "step 12000, time 581s, loss 0.08\n",
            "step 12100, time 586s, loss 0.08\n",
            "step 12200, time 591s, loss 0.08\n",
            "step 12300, time 595s, loss 0.07\n",
            "step 12400, time 600s, loss 0.08\n",
            "step 12500, time 605s, loss 0.10\n",
            "step 12600, time 610s, loss 0.09\n",
            "step 12700, time 614s, loss 0.08\n",
            "step 12800, time 619s, loss 0.08\n",
            "step 12900, time 624s, loss 0.08\n",
            "step 13000, time 629s, loss 0.09\n",
            "step 13100, time 634s, loss 0.07\n",
            "step 13200, time 639s, loss 0.10\n",
            "step 13300, time 643s, loss 0.10\n",
            "step 13400, time 648s, loss 0.10\n",
            "step 13500, time 653s, loss 0.08\n",
            "step 13600, time 658s, loss 0.10\n",
            "step 13700, time 663s, loss 0.09\n",
            "step 13800, time 668s, loss 0.08\n",
            "step 13900, time 672s, loss 0.08\n",
            "step 14000, time 677s, loss 0.09\n",
            "step 14100, time 682s, loss 0.08\n",
            "step 14200, time 687s, loss 0.10\n",
            "step 14300, time 692s, loss 0.08\n",
            "step 14400, time 697s, loss 0.09\n",
            "step 14500, time 702s, loss 0.08\n",
            "step 14600, time 707s, loss 0.09\n",
            "step 14700, time 712s, loss 0.06\n",
            "step 14800, time 717s, loss 0.09\n",
            "step 14900, time 722s, loss 0.08\n",
            "step 15000, time 727s, loss 0.10\n",
            "step 15100, time 732s, loss 0.07\n",
            "step 15200, time 737s, loss 0.07\n",
            "step 15300, time 742s, loss 0.10\n",
            "step 15400, time 747s, loss 0.08\n",
            "step 15500, time 751s, loss 0.07\n",
            "step 15600, time 756s, loss 0.07\n",
            "step 15700, time 761s, loss 0.09\n",
            "step 15800, time 766s, loss 0.09\n",
            "step 15900, time 771s, loss 0.09\n",
            "step 16000, time 776s, loss 0.07\n",
            "step 16100, time 780s, loss 0.08\n",
            "step 16200, time 785s, loss 0.08\n",
            "step 16300, time 790s, loss 0.09\n",
            "step 16400, time 794s, loss 0.07\n",
            "step 16500, time 799s, loss 0.06\n",
            "step 16600, time 804s, loss 0.07\n",
            "step 16700, time 808s, loss 0.07\n",
            "step 16800, time 813s, loss 0.06\n",
            "step 16900, time 818s, loss 0.09\n",
            "step 17000, time 823s, loss 0.09\n",
            "step 17100, time 828s, loss 0.07\n",
            "step 17200, time 833s, loss 0.08\n",
            "step 17300, time 838s, loss 0.09\n",
            "step 17400, time 843s, loss 0.09\n",
            "step 17500, time 848s, loss 0.09\n",
            "step 17600, time 853s, loss 0.10\n",
            "step 17700, time 858s, loss 0.08\n",
            "step 17800, time 863s, loss 0.08\n",
            "step 17900, time 868s, loss 0.10\n",
            "step 18000, time 873s, loss 0.09\n",
            "step 18100, time 878s, loss 0.08\n",
            "step 18200, time 883s, loss 0.08\n",
            "step 18300, time 888s, loss 0.07\n",
            "step 18400, time 893s, loss 0.06\n",
            "step 18500, time 898s, loss 0.08\n",
            "step 18600, time 903s, loss 0.08\n",
            "step 18700, time 907s, loss 0.08\n",
            "step 18800, time 912s, loss 0.08\n",
            "step 18900, time 917s, loss 0.06\n",
            "step 19000, time 922s, loss 0.08\n",
            "step 19100, time 927s, loss 0.11\n",
            "step 19200, time 932s, loss 0.08\n",
            "step 19300, time 937s, loss 0.08\n",
            "step 19400, time 941s, loss 0.08\n",
            "step 19500, time 946s, loss 0.05\n",
            "step 19600, time 951s, loss 0.09\n",
            "step 19700, time 956s, loss 0.07\n",
            "step 19800, time 961s, loss 0.07\n",
            "step 19900, time 965s, loss 0.09\n",
            "step 20000, time 970s, loss 0.08\n",
            "step 20100, time 975s, loss 0.09\n",
            "step 20200, time 980s, loss 0.08\n",
            "step 20300, time 985s, loss 0.05\n",
            "step 20400, time 990s, loss 0.09\n",
            "step 20500, time 994s, loss 0.07\n",
            "step 20600, time 999s, loss 0.08\n",
            "step 20700, time 1004s, loss 0.08\n",
            "step 20800, time 1009s, loss 0.08\n",
            "step 20900, time 1014s, loss 0.06\n",
            "step 21000, time 1018s, loss 0.07\n",
            "step 21100, time 1023s, loss 0.07\n",
            "step 21200, time 1028s, loss 0.07\n",
            "step 21300, time 1033s, loss 0.10\n",
            "step 21400, time 1038s, loss 0.07\n",
            "step 21500, time 1043s, loss 0.09\n",
            "step 21600, time 1048s, loss 0.07\n",
            "step 21700, time 1053s, loss 0.07\n",
            "step 21800, time 1057s, loss 0.08\n",
            "step 21900, time 1062s, loss 0.07\n",
            "step 22000, time 1067s, loss 0.10\n",
            "Saving classifier result at: /content/drive/MyDrive/DualRL/tmp/cls_result_yelp.txt\n",
            "Dev accuracy: 97.12\n",
            "Saving model to:  /content/drive/MyDrive/DualRL/tmp/model/yelp/cls/\n",
            "Saving classifier result at: /content/drive/MyDrive/DualRL/tmp/cls_result_yelp.txt\n",
            "Test accuracy: 96.20\n",
            "--------------------Epoch 2--------------------\n",
            "step 22100, time 1080s, loss 0.07\n",
            "step 22200, time 1085s, loss 0.08\n",
            "step 22300, time 1090s, loss 0.07\n",
            "step 22400, time 1095s, loss 0.08\n",
            "step 22500, time 1100s, loss 0.07\n",
            "step 22600, time 1104s, loss 0.09\n",
            "step 22700, time 1109s, loss 0.06\n",
            "step 22800, time 1114s, loss 0.08\n",
            "step 22900, time 1118s, loss 0.07\n",
            "step 23000, time 1123s, loss 0.07\n",
            "step 23100, time 1128s, loss 0.09\n",
            "step 23200, time 1133s, loss 0.07\n",
            "step 23300, time 1138s, loss 0.07\n",
            "step 23400, time 1142s, loss 0.08\n",
            "step 23500, time 1147s, loss 0.07\n",
            "step 23600, time 1152s, loss 0.05\n",
            "step 23700, time 1157s, loss 0.09\n",
            "step 23800, time 1161s, loss 0.07\n",
            "step 23900, time 1166s, loss 0.05\n",
            "step 24000, time 1171s, loss 0.09\n",
            "step 24100, time 1176s, loss 0.06\n",
            "step 24200, time 1181s, loss 0.08\n",
            "step 24300, time 1185s, loss 0.06\n",
            "step 24400, time 1190s, loss 0.06\n",
            "step 24500, time 1195s, loss 0.06\n",
            "step 24600, time 1200s, loss 0.07\n",
            "step 24700, time 1204s, loss 0.05\n",
            "step 24800, time 1209s, loss 0.06\n",
            "step 24900, time 1214s, loss 0.06\n",
            "step 25000, time 1219s, loss 0.07\n",
            "step 25100, time 1224s, loss 0.08\n",
            "step 25200, time 1229s, loss 0.07\n",
            "step 25300, time 1234s, loss 0.07\n",
            "step 25400, time 1239s, loss 0.07\n",
            "step 25500, time 1244s, loss 0.07\n",
            "step 25600, time 1249s, loss 0.08\n",
            "step 25700, time 1255s, loss 0.08\n",
            "step 25800, time 1260s, loss 0.07\n",
            "step 25900, time 1265s, loss 0.07\n",
            "step 26000, time 1270s, loss 0.08\n",
            "step 26100, time 1275s, loss 0.07\n",
            "step 26200, time 1280s, loss 0.06\n",
            "step 26300, time 1285s, loss 0.07\n",
            "step 26400, time 1290s, loss 0.08\n",
            "step 26500, time 1295s, loss 0.06\n",
            "step 26600, time 1300s, loss 0.07\n",
            "step 26700, time 1304s, loss 0.09\n",
            "step 26800, time 1309s, loss 0.07\n",
            "step 26900, time 1314s, loss 0.08\n",
            "step 27000, time 1319s, loss 0.07\n",
            "step 27100, time 1324s, loss 0.06\n",
            "step 27200, time 1329s, loss 0.06\n",
            "step 27300, time 1334s, loss 0.07\n",
            "step 27400, time 1339s, loss 0.09\n",
            "step 27500, time 1344s, loss 0.07\n",
            "step 27600, time 1349s, loss 0.06\n",
            "step 27700, time 1354s, loss 0.07\n",
            "step 27800, time 1358s, loss 0.05\n",
            "step 27900, time 1363s, loss 0.09\n",
            "step 28000, time 1368s, loss 0.07\n",
            "step 28100, time 1373s, loss 0.06\n",
            "step 28200, time 1378s, loss 0.06\n",
            "step 28300, time 1382s, loss 0.06\n",
            "step 28400, time 1387s, loss 0.07\n",
            "step 28500, time 1392s, loss 0.08\n",
            "step 28600, time 1397s, loss 0.04\n",
            "step 28700, time 1402s, loss 0.06\n",
            "step 28800, time 1407s, loss 0.06\n",
            "step 28900, time 1411s, loss 0.09\n",
            "step 29000, time 1416s, loss 0.06\n",
            "step 29100, time 1421s, loss 0.05\n",
            "step 29200, time 1426s, loss 0.07\n",
            "step 29300, time 1431s, loss 0.06\n",
            "step 29400, time 1436s, loss 0.07\n",
            "step 29500, time 1441s, loss 0.08\n",
            "step 29600, time 1446s, loss 0.07\n",
            "step 29700, time 1451s, loss 0.06\n",
            "step 29800, time 1456s, loss 0.06\n",
            "step 29900, time 1460s, loss 0.05\n",
            "step 30000, time 1465s, loss 0.06\n",
            "step 30100, time 1470s, loss 0.05\n",
            "step 30200, time 1475s, loss 0.06\n",
            "step 30300, time 1480s, loss 0.06\n",
            "step 30400, time 1485s, loss 0.05\n",
            "step 30500, time 1489s, loss 0.06\n",
            "step 30600, time 1494s, loss 0.06\n",
            "step 30700, time 1499s, loss 0.08\n",
            "step 30800, time 1504s, loss 0.04\n",
            "step 30900, time 1509s, loss 0.05\n",
            "step 31000, time 1514s, loss 0.08\n",
            "step 31100, time 1519s, loss 0.06\n",
            "step 31200, time 1524s, loss 0.06\n",
            "step 31300, time 1529s, loss 0.06\n",
            "step 31400, time 1534s, loss 0.06\n",
            "step 31500, time 1539s, loss 0.06\n",
            "step 31600, time 1543s, loss 0.05\n",
            "step 31700, time 1548s, loss 0.06\n",
            "step 31800, time 1554s, loss 0.08\n",
            "step 31900, time 1559s, loss 0.05\n",
            "step 32000, time 1564s, loss 0.07\n",
            "step 32100, time 1569s, loss 0.05\n",
            "step 32200, time 1574s, loss 0.08\n",
            "step 32300, time 1579s, loss 0.07\n",
            "step 32400, time 1584s, loss 0.06\n",
            "step 32500, time 1589s, loss 0.05\n",
            "step 32600, time 1594s, loss 0.08\n",
            "step 32700, time 1599s, loss 0.07\n",
            "step 32800, time 1604s, loss 0.07\n",
            "step 32900, time 1609s, loss 0.05\n",
            "step 33000, time 1614s, loss 0.06\n",
            "step 33100, time 1619s, loss 0.07\n",
            "step 33200, time 1624s, loss 0.06\n",
            "step 33300, time 1629s, loss 0.07\n",
            "step 33400, time 1634s, loss 0.06\n",
            "step 33500, time 1639s, loss 0.06\n",
            "step 33600, time 1644s, loss 0.04\n",
            "step 33700, time 1649s, loss 0.07\n",
            "step 33800, time 1654s, loss 0.08\n",
            "step 33900, time 1658s, loss 0.06\n",
            "step 34000, time 1663s, loss 0.06\n",
            "step 34100, time 1668s, loss 0.05\n",
            "step 34200, time 1673s, loss 0.07\n",
            "step 34300, time 1678s, loss 0.05\n",
            "step 34400, time 1683s, loss 0.06\n",
            "step 34500, time 1688s, loss 0.05\n",
            "step 34600, time 1693s, loss 0.07\n",
            "step 34700, time 1698s, loss 0.07\n",
            "step 34800, time 1703s, loss 0.08\n",
            "step 34900, time 1708s, loss 0.04\n",
            "step 35000, time 1712s, loss 0.07\n",
            "step 35100, time 1717s, loss 0.07\n",
            "step 35200, time 1723s, loss 0.06\n",
            "step 35300, time 1727s, loss 0.08\n",
            "step 35400, time 1732s, loss 0.06\n",
            "step 35500, time 1737s, loss 0.06\n",
            "step 35600, time 1742s, loss 0.06\n",
            "step 35700, time 1747s, loss 0.07\n",
            "step 35800, time 1752s, loss 0.07\n",
            "step 35900, time 1757s, loss 0.05\n",
            "step 36000, time 1762s, loss 0.07\n",
            "step 36100, time 1767s, loss 0.06\n",
            "step 36200, time 1772s, loss 0.08\n",
            "step 36300, time 1777s, loss 0.07\n",
            "step 36400, time 1782s, loss 0.08\n",
            "step 36500, time 1786s, loss 0.06\n",
            "step 36600, time 1791s, loss 0.07\n",
            "step 36700, time 1796s, loss 0.06\n",
            "step 36800, time 1801s, loss 0.06\n",
            "step 36900, time 1806s, loss 0.06\n",
            "step 37000, time 1811s, loss 0.06\n",
            "step 37100, time 1816s, loss 0.07\n",
            "step 37200, time 1821s, loss 0.04\n",
            "step 37300, time 1826s, loss 0.07\n",
            "step 37400, time 1831s, loss 0.07\n",
            "step 37500, time 1835s, loss 0.05\n",
            "step 37600, time 1840s, loss 0.07\n",
            "step 37700, time 1845s, loss 0.06\n",
            "step 37800, time 1850s, loss 0.08\n",
            "step 37900, time 1855s, loss 0.07\n",
            "step 38000, time 1860s, loss 0.05\n",
            "step 38100, time 1865s, loss 0.05\n",
            "step 38200, time 1870s, loss 0.07\n",
            "step 38300, time 1875s, loss 0.06\n",
            "step 38400, time 1880s, loss 0.07\n",
            "step 38500, time 1885s, loss 0.05\n",
            "step 38600, time 1890s, loss 0.05\n",
            "step 38700, time 1895s, loss 0.05\n",
            "step 38800, time 1900s, loss 0.05\n",
            "step 38900, time 1905s, loss 0.06\n",
            "step 39000, time 1910s, loss 0.05\n",
            "step 39100, time 1915s, loss 0.07\n",
            "step 39200, time 1920s, loss 0.05\n",
            "step 39300, time 1925s, loss 0.05\n",
            "step 39400, time 1930s, loss 0.08\n",
            "step 39500, time 1934s, loss 0.07\n",
            "step 39600, time 1939s, loss 0.09\n",
            "step 39700, time 1944s, loss 0.07\n",
            "step 39800, time 1949s, loss 0.06\n",
            "step 39900, time 1954s, loss 0.08\n",
            "step 40000, time 1959s, loss 0.08\n",
            "step 40100, time 1964s, loss 0.06\n",
            "step 40200, time 1969s, loss 0.06\n",
            "step 40300, time 1974s, loss 0.05\n",
            "step 40400, time 1979s, loss 0.06\n",
            "step 40500, time 1984s, loss 0.06\n",
            "step 40600, time 1989s, loss 0.06\n",
            "step 40700, time 1994s, loss 0.08\n",
            "step 40800, time 1999s, loss 0.06\n",
            "step 40900, time 2004s, loss 0.05\n",
            "step 41000, time 2009s, loss 0.05\n",
            "step 41100, time 2014s, loss 0.07\n",
            "step 41200, time 2019s, loss 0.07\n",
            "step 41300, time 2023s, loss 0.05\n",
            "step 41400, time 2028s, loss 0.06\n",
            "step 41500, time 2033s, loss 0.07\n",
            "step 41600, time 2038s, loss 0.05\n",
            "step 41700, time 2043s, loss 0.06\n",
            "step 41800, time 2048s, loss 0.07\n",
            "step 41900, time 2053s, loss 0.07\n",
            "step 42000, time 2058s, loss 0.07\n",
            "step 42100, time 2063s, loss 0.07\n",
            "step 42200, time 2068s, loss 0.06\n",
            "step 42300, time 2073s, loss 0.04\n",
            "step 42400, time 2078s, loss 0.06\n",
            "step 42500, time 2083s, loss 0.07\n",
            "step 42600, time 2088s, loss 0.05\n",
            "step 42700, time 2093s, loss 0.08\n",
            "step 42800, time 2098s, loss 0.06\n",
            "step 42900, time 2103s, loss 0.05\n",
            "step 43000, time 2108s, loss 0.05\n",
            "step 43100, time 2113s, loss 0.06\n",
            "step 43200, time 2118s, loss 0.06\n",
            "step 43300, time 2123s, loss 0.05\n",
            "step 43400, time 2128s, loss 0.07\n",
            "step 43500, time 2133s, loss 0.07\n",
            "step 43600, time 2138s, loss 0.07\n",
            "step 43700, time 2143s, loss 0.05\n",
            "step 43800, time 2148s, loss 0.07\n",
            "step 43900, time 2153s, loss 0.06\n",
            "step 44000, time 2158s, loss 0.08\n",
            "step 44100, time 2163s, loss 0.05\n",
            "Saving classifier result at: /content/drive/MyDrive/DualRL/tmp/cls_result_yelp.txt\n",
            "Dev accuracy: 97.50\n",
            "Saving model to:  /content/drive/MyDrive/DualRL/tmp/model/yelp/cls/\n",
            "Saving classifier result at: /content/drive/MyDrive/DualRL/tmp/cls_result_yelp.txt\n",
            "Test accuracy: 96.90\n",
            "--------------------Epoch 3--------------------\n",
            "step 44200, time 2176s, loss 0.06\n",
            "step 44300, time 2181s, loss 0.05\n",
            "step 44400, time 2186s, loss 0.06\n",
            "step 44500, time 2191s, loss 0.06\n",
            "step 44600, time 2196s, loss 0.06\n",
            "step 44700, time 2201s, loss 0.06\n",
            "step 44800, time 2206s, loss 0.06\n",
            "step 44900, time 2211s, loss 0.06\n",
            "step 45000, time 2216s, loss 0.07\n",
            "step 45100, time 2221s, loss 0.07\n",
            "step 45200, time 2226s, loss 0.07\n",
            "step 45300, time 2231s, loss 0.07\n",
            "step 45400, time 2236s, loss 0.06\n",
            "step 45500, time 2241s, loss 0.06\n",
            "step 45600, time 2246s, loss 0.05\n",
            "step 45700, time 2251s, loss 0.07\n",
            "step 45800, time 2256s, loss 0.07\n",
            "step 45900, time 2261s, loss 0.04\n",
            "step 46000, time 2266s, loss 0.06\n",
            "step 46100, time 2271s, loss 0.05\n",
            "step 46200, time 2276s, loss 0.07\n",
            "step 46300, time 2280s, loss 0.05\n",
            "step 46400, time 2285s, loss 0.05\n",
            "step 46500, time 2290s, loss 0.06\n",
            "step 46600, time 2295s, loss 0.05\n",
            "step 46700, time 2300s, loss 0.06\n",
            "step 46800, time 2305s, loss 0.04\n",
            "step 46900, time 2310s, loss 0.05\n",
            "step 47000, time 2315s, loss 0.05\n",
            "step 47100, time 2320s, loss 0.06\n",
            "step 47200, time 2325s, loss 0.05\n",
            "step 47300, time 2331s, loss 0.07\n",
            "step 47400, time 2336s, loss 0.05\n",
            "step 47500, time 2341s, loss 0.05\n",
            "step 47600, time 2346s, loss 0.08\n",
            "step 47700, time 2351s, loss 0.06\n",
            "step 47800, time 2356s, loss 0.07\n",
            "step 47900, time 2361s, loss 0.06\n",
            "step 48000, time 2366s, loss 0.06\n",
            "step 48100, time 2371s, loss 0.06\n",
            "step 48200, time 2376s, loss 0.06\n",
            "step 48300, time 2381s, loss 0.04\n",
            "step 48400, time 2386s, loss 0.06\n",
            "step 48500, time 2391s, loss 0.05\n",
            "step 48600, time 2396s, loss 0.06\n",
            "step 48700, time 2401s, loss 0.07\n",
            "step 48800, time 2406s, loss 0.06\n",
            "step 48900, time 2411s, loss 0.06\n",
            "step 49000, time 2415s, loss 0.08\n",
            "step 49100, time 2420s, loss 0.06\n",
            "step 49200, time 2425s, loss 0.04\n",
            "step 49300, time 2430s, loss 0.06\n",
            "step 49400, time 2435s, loss 0.06\n",
            "step 49500, time 2440s, loss 0.08\n",
            "step 49600, time 2445s, loss 0.05\n",
            "step 49700, time 2450s, loss 0.05\n",
            "step 49800, time 2455s, loss 0.05\n",
            "step 49900, time 2460s, loss 0.05\n",
            "step 50000, time 2465s, loss 0.07\n",
            "step 50100, time 2469s, loss 0.06\n",
            "step 50200, time 2474s, loss 0.04\n",
            "step 50300, time 2479s, loss 0.06\n",
            "step 50400, time 2484s, loss 0.04\n",
            "step 50500, time 2489s, loss 0.06\n",
            "step 50600, time 2494s, loss 0.06\n",
            "step 50700, time 2499s, loss 0.05\n",
            "step 50800, time 2504s, loss 0.06\n",
            "step 50900, time 2509s, loss 0.05\n",
            "step 51000, time 2514s, loss 0.07\n",
            "step 51100, time 2519s, loss 0.05\n",
            "step 51200, time 2524s, loss 0.07\n",
            "step 51300, time 2529s, loss 0.05\n",
            "step 51400, time 2534s, loss 0.06\n",
            "step 51500, time 2539s, loss 0.05\n",
            "step 51600, time 2544s, loss 0.08\n",
            "step 51700, time 2549s, loss 0.05\n",
            "step 51800, time 2555s, loss 0.06\n",
            "step 51900, time 2560s, loss 0.06\n",
            "step 52000, time 2565s, loss 0.06\n",
            "step 52100, time 2570s, loss 0.06\n",
            "step 52200, time 2575s, loss 0.05\n",
            "step 52300, time 2580s, loss 0.05\n",
            "step 52400, time 2585s, loss 0.04\n",
            "step 52500, time 2590s, loss 0.05\n",
            "step 52600, time 2595s, loss 0.05\n",
            "step 52700, time 2599s, loss 0.06\n",
            "step 52800, time 2605s, loss 0.04\n",
            "step 52900, time 2610s, loss 0.03\n",
            "step 53000, time 2614s, loss 0.05\n",
            "step 53100, time 2619s, loss 0.06\n",
            "step 53200, time 2624s, loss 0.05\n",
            "step 53300, time 2629s, loss 0.05\n",
            "step 53400, time 2634s, loss 0.06\n",
            "step 53500, time 2639s, loss 0.05\n",
            "step 53600, time 2644s, loss 0.06\n",
            "step 53700, time 2648s, loss 0.04\n",
            "step 53800, time 2653s, loss 0.06\n",
            "step 53900, time 2658s, loss 0.05\n",
            "step 54000, time 2663s, loss 0.06\n",
            "step 54100, time 2668s, loss 0.05\n",
            "step 54200, time 2673s, loss 0.04\n",
            "step 54300, time 2678s, loss 0.06\n",
            "step 54400, time 2683s, loss 0.06\n",
            "step 54500, time 2688s, loss 0.05\n",
            "step 54600, time 2693s, loss 0.05\n",
            "step 54700, time 2699s, loss 0.07\n",
            "step 54800, time 2703s, loss 0.05\n",
            "step 54900, time 2708s, loss 0.05\n",
            "step 55000, time 2713s, loss 0.05\n",
            "step 55100, time 2718s, loss 0.06\n",
            "step 55200, time 2723s, loss 0.06\n",
            "step 55300, time 2728s, loss 0.05\n",
            "step 55400, time 2733s, loss 0.05\n",
            "step 55500, time 2738s, loss 0.05\n",
            "step 55600, time 2743s, loss 0.05\n",
            "step 55700, time 2748s, loss 0.05\n",
            "step 55800, time 2753s, loss 0.08\n",
            "step 55900, time 2758s, loss 0.06\n",
            "step 56000, time 2763s, loss 0.06\n",
            "step 56100, time 2768s, loss 0.05\n",
            "step 56200, time 2773s, loss 0.05\n",
            "step 56300, time 2778s, loss 0.06\n",
            "step 56400, time 2783s, loss 0.04\n",
            "step 56500, time 2788s, loss 0.05\n",
            "step 56600, time 2793s, loss 0.06\n",
            "step 56700, time 2798s, loss 0.05\n",
            "step 56800, time 2802s, loss 0.05\n",
            "step 56900, time 2807s, loss 0.07\n",
            "step 57000, time 2812s, loss 0.04\n",
            "step 57100, time 2817s, loss 0.06\n",
            "step 57200, time 2822s, loss 0.05\n",
            "step 57300, time 2827s, loss 0.07\n",
            "step 57400, time 2832s, loss 0.06\n",
            "step 57500, time 2837s, loss 0.05\n",
            "step 57600, time 2842s, loss 0.04\n",
            "step 57700, time 2847s, loss 0.07\n",
            "step 57800, time 2851s, loss 0.05\n",
            "step 57900, time 2856s, loss 0.07\n",
            "step 58000, time 2861s, loss 0.05\n",
            "step 58100, time 2866s, loss 0.06\n",
            "step 58200, time 2871s, loss 0.06\n",
            "step 58300, time 2876s, loss 0.06\n",
            "step 58400, time 2881s, loss 0.06\n",
            "step 58500, time 2886s, loss 0.06\n",
            "step 58600, time 2890s, loss 0.05\n",
            "step 58700, time 2895s, loss 0.06\n",
            "step 58800, time 2900s, loss 0.04\n",
            "step 58900, time 2905s, loss 0.06\n",
            "step 59000, time 2910s, loss 0.04\n",
            "step 59100, time 2915s, loss 0.06\n",
            "step 59200, time 2920s, loss 0.04\n",
            "step 59300, time 2925s, loss 0.04\n",
            "step 59400, time 2930s, loss 0.06\n",
            "step 59500, time 2935s, loss 0.06\n",
            "step 59600, time 2940s, loss 0.05\n",
            "step 59700, time 2945s, loss 0.04\n",
            "step 59800, time 2950s, loss 0.05\n",
            "step 59900, time 2955s, loss 0.07\n",
            "step 60000, time 2960s, loss 0.06\n",
            "step 60100, time 2965s, loss 0.04\n",
            "step 60200, time 2970s, loss 0.04\n",
            "step 60300, time 2974s, loss 0.04\n",
            "step 60400, time 2979s, loss 0.05\n",
            "step 60500, time 2984s, loss 0.05\n",
            "step 60600, time 2989s, loss 0.04\n",
            "step 60700, time 2994s, loss 0.06\n",
            "step 60800, time 2999s, loss 0.04\n",
            "step 60900, time 3004s, loss 0.04\n",
            "step 61000, time 3009s, loss 0.05\n",
            "step 61100, time 3014s, loss 0.05\n",
            "step 61200, time 3018s, loss 0.04\n",
            "step 61300, time 3023s, loss 0.05\n",
            "step 61400, time 3028s, loss 0.05\n",
            "step 61500, time 3033s, loss 0.07\n",
            "step 61600, time 3038s, loss 0.07\n",
            "step 61700, time 3043s, loss 0.07\n",
            "step 61800, time 3048s, loss 0.04\n",
            "step 61900, time 3053s, loss 0.05\n",
            "step 62000, time 3058s, loss 0.06\n",
            "step 62100, time 3063s, loss 0.07\n",
            "step 62200, time 3068s, loss 0.05\n",
            "step 62300, time 3073s, loss 0.04\n",
            "step 62400, time 3077s, loss 0.04\n",
            "step 62500, time 3083s, loss 0.04\n",
            "step 62600, time 3087s, loss 0.05\n",
            "step 62700, time 3092s, loss 0.06\n",
            "step 62800, time 3097s, loss 0.07\n",
            "step 62900, time 3102s, loss 0.06\n",
            "step 63000, time 3107s, loss 0.03\n",
            "step 63100, time 3112s, loss 0.04\n",
            "step 63200, time 3117s, loss 0.10\n",
            "step 63300, time 3122s, loss 0.04\n",
            "step 63400, time 3126s, loss 0.06\n",
            "step 63500, time 3131s, loss 0.04\n",
            "step 63600, time 3136s, loss 0.05\n",
            "step 63700, time 3141s, loss 0.06\n",
            "step 63800, time 3146s, loss 0.05\n",
            "step 63900, time 3150s, loss 0.06\n",
            "step 64000, time 3155s, loss 0.06\n",
            "step 64100, time 3160s, loss 0.07\n",
            "step 64200, time 3165s, loss 0.06\n",
            "step 64300, time 3170s, loss 0.05\n",
            "step 64400, time 3175s, loss 0.03\n",
            "step 64500, time 3179s, loss 0.06\n",
            "step 64600, time 3184s, loss 0.04\n",
            "step 64700, time 3189s, loss 0.05\n",
            "step 64800, time 3194s, loss 0.07\n",
            "step 64900, time 3199s, loss 0.04\n",
            "step 65000, time 3203s, loss 0.04\n",
            "step 65100, time 3208s, loss 0.05\n",
            "step 65200, time 3213s, loss 0.04\n",
            "step 65300, time 3218s, loss 0.03\n",
            "step 65400, time 3223s, loss 0.08\n",
            "step 65500, time 3228s, loss 0.05\n",
            "step 65600, time 3233s, loss 0.05\n",
            "step 65700, time 3238s, loss 0.05\n",
            "step 65800, time 3242s, loss 0.06\n",
            "step 65900, time 3247s, loss 0.06\n",
            "step 66000, time 3252s, loss 0.05\n",
            "step 66100, time 3257s, loss 0.07\n",
            "Saving classifier result at: /content/drive/MyDrive/DualRL/tmp/cls_result_yelp.txt\n",
            "Dev accuracy: 97.38\n",
            "Saving classifier result at: /content/drive/MyDrive/DualRL/tmp/cls_result_yelp.txt\n",
            "Test accuracy: 96.70\n",
            "--------------------Epoch 4--------------------\n",
            "step 66200, time 3269s, loss 0.04\n",
            "step 66300, time 3274s, loss 0.06\n",
            "step 66400, time 3279s, loss 0.05\n",
            "step 66500, time 3284s, loss 0.05\n",
            "step 66600, time 3289s, loss 0.04\n",
            "step 66700, time 3294s, loss 0.06\n",
            "step 66800, time 3298s, loss 0.04\n",
            "step 66900, time 3303s, loss 0.06\n",
            "step 67000, time 3308s, loss 0.04\n",
            "step 67100, time 3313s, loss 0.06\n",
            "step 67200, time 3318s, loss 0.08\n",
            "step 67300, time 3323s, loss 0.07\n",
            "step 67400, time 3328s, loss 0.05\n",
            "step 67500, time 3333s, loss 0.06\n",
            "step 67600, time 3338s, loss 0.05\n",
            "step 67700, time 3343s, loss 0.04\n",
            "step 67800, time 3347s, loss 0.07\n",
            "step 67900, time 3352s, loss 0.05\n",
            "step 68000, time 3357s, loss 0.04\n",
            "step 68100, time 3362s, loss 0.06\n",
            "step 68200, time 3367s, loss 0.05\n",
            "step 68300, time 3371s, loss 0.06\n",
            "step 68400, time 3376s, loss 0.05\n",
            "step 68500, time 3381s, loss 0.05\n",
            "step 68600, time 3386s, loss 0.04\n",
            "step 68700, time 3390s, loss 0.04\n",
            "step 68800, time 3395s, loss 0.04\n",
            "step 68900, time 3400s, loss 0.04\n",
            "step 69000, time 3405s, loss 0.06\n",
            "step 69100, time 3410s, loss 0.05\n",
            "step 69200, time 3415s, loss 0.06\n",
            "step 69300, time 3419s, loss 0.05\n",
            "step 69400, time 3424s, loss 0.05\n",
            "step 69500, time 3429s, loss 0.05\n",
            "step 69600, time 3434s, loss 0.05\n",
            "step 69700, time 3439s, loss 0.07\n",
            "step 69800, time 3444s, loss 0.06\n",
            "step 69900, time 3449s, loss 0.06\n",
            "step 70000, time 3454s, loss 0.05\n",
            "step 70100, time 3459s, loss 0.05\n",
            "step 70200, time 3464s, loss 0.05\n",
            "step 70300, time 3468s, loss 0.04\n",
            "step 70400, time 3473s, loss 0.04\n",
            "step 70500, time 3478s, loss 0.05\n",
            "step 70600, time 3483s, loss 0.04\n",
            "step 70700, time 3488s, loss 0.04\n",
            "step 70800, time 3493s, loss 0.07\n",
            "step 70900, time 3498s, loss 0.05\n",
            "step 71000, time 3502s, loss 0.06\n",
            "step 71100, time 3507s, loss 0.05\n",
            "step 71200, time 3512s, loss 0.05\n",
            "step 71300, time 3517s, loss 0.05\n",
            "step 71400, time 3522s, loss 0.05\n",
            "step 71500, time 3527s, loss 0.07\n",
            "step 71600, time 3532s, loss 0.06\n",
            "step 71700, time 3537s, loss 0.05\n",
            "step 71800, time 3541s, loss 0.05\n",
            "step 71900, time 3546s, loss 0.03\n",
            "step 72000, time 3551s, loss 0.05\n",
            "step 72100, time 3556s, loss 0.06\n",
            "step 72200, time 3561s, loss 0.03\n",
            "step 72300, time 3566s, loss 0.04\n",
            "step 72400, time 3570s, loss 0.04\n",
            "step 72500, time 3575s, loss 0.04\n",
            "step 72600, time 3580s, loss 0.06\n",
            "step 72700, time 3585s, loss 0.03\n",
            "step 72800, time 3590s, loss 0.06\n",
            "step 72900, time 3595s, loss 0.03\n",
            "step 73000, time 3600s, loss 0.05\n",
            "step 73100, time 3604s, loss 0.06\n",
            "step 73200, time 3609s, loss 0.03\n",
            "step 73300, time 3614s, loss 0.04\n",
            "step 73400, time 3619s, loss 0.04\n",
            "step 73500, time 3624s, loss 0.06\n",
            "step 73600, time 3629s, loss 0.06\n",
            "step 73700, time 3634s, loss 0.06\n",
            "step 73800, time 3639s, loss 0.04\n",
            "step 73900, time 3644s, loss 0.05\n",
            "step 74000, time 3649s, loss 0.05\n",
            "step 74100, time 3653s, loss 0.04\n",
            "step 74200, time 3658s, loss 0.04\n",
            "step 74300, time 3663s, loss 0.04\n",
            "step 74400, time 3668s, loss 0.04\n",
            "step 74500, time 3673s, loss 0.04\n",
            "step 74600, time 3678s, loss 0.05\n",
            "step 74700, time 3683s, loss 0.05\n",
            "step 74800, time 3687s, loss 0.04\n",
            "step 74900, time 3692s, loss 0.02\n",
            "step 75000, time 3697s, loss 0.03\n",
            "step 75100, time 3702s, loss 0.07\n",
            "step 75200, time 3707s, loss 0.04\n",
            "step 75300, time 3712s, loss 0.04\n",
            "step 75400, time 3717s, loss 0.04\n",
            "step 75500, time 3721s, loss 0.05\n",
            "step 75600, time 3726s, loss 0.04\n",
            "step 75700, time 3731s, loss 0.04\n",
            "step 75800, time 3735s, loss 0.05\n",
            "step 75900, time 3740s, loss 0.05\n",
            "step 76000, time 3745s, loss 0.04\n",
            "step 76100, time 3750s, loss 0.05\n",
            "step 76200, time 3755s, loss 0.04\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQTyTwXyz4ZM"
      },
      "source": [
        "https://newbedev.com/how-to-fix-ipykernel-launcher-py-error-unrecognized-arguments-in-jupyter\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeaaPadN2d4X"
      },
      "source": [
        "https://medium.com/analytics-vidhya/importing-your-own-python-module-or-python-file-into-colab-3e365f0a35ec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUeidUHylO90"
      },
      "source": [
        "https://github.com/makinacorpus/easydict/blob/master/easydict/__init__.py"
      ]
    }
  ]
}